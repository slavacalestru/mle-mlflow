{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e371ca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "# from sklearn.metrics \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "TRACKING_SERVER_HOST = \"127.0.0.1\"\n",
    "TRACKING_SERVER_PORT = 5000\n",
    "YOUR_NAME = \"Slava\"\n",
    "assert YOUR_NAME, \"enter your name\"\n",
    "\n",
    "EXPERIMENT_NAME = f\"test_connection_experiment_{YOUR_NAME}\"\n",
    "RUN_NAME = f\"test_connection_run\"\n",
    "\n",
    "METRIC_NAME = \"test_metric\"\n",
    "METRIC_VALUE = 0\n",
    "\n",
    "mlflow.set_tracking_uri(f\"http://{TRACKING_SERVER_HOST}:{TRACKING_SERVER_PORT}\")\n",
    "\n",
    "experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "with mlflow.start_run(run_name=RUN_NAME, experiment_id=experiment_id) as run:\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "    mlflow.log_metric(METRIC_NAME, METRIC_VALUE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49420ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "run = mlflow.get_run(run_id)\n",
    "\n",
    "assert \"active\" == experiment.lifecycle_stage\n",
    "assert mlflow.get_run(run_id)\n",
    "assert METRIC_VALUE == run.data.metrics[METRIC_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbebabf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "872da549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['catboost_classifier.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import mlflow\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, confusion_matrix, log_loss\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "np.random.seed(43)\n",
    "\n",
    "n_samples = 1000\n",
    "\n",
    "X = pd.DataFrame({\n",
    "    \"feature_1\": np.random.randn(n_samples),\n",
    "    \"feature_2\": np.random.randn(n_samples),\n",
    "    \"feature_3\": np.random.randn(n_samples),\n",
    "})\n",
    "\n",
    "\n",
    "y = (\n",
    "    (X[\"feature_1\"] + 0.5 * X[\"feature_2\"] - 0.3 * X[\"feature_3\"]) > 0\n",
    ").astype(int)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    iterations=200,\n",
    "    depth=6,\n",
    "    learning_rate=0.1,\n",
    "    loss_function=\"Logloss\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "probas = model.predict_proba(X_test)\n",
    "\n",
    "model_path = \"catboost_classifier.pkl\"\n",
    "X_train.to_csv(\"train_data.csv\", index=False)\n",
    "X_test.to_csv(\"test_data.csv\", index=False)\n",
    "\n",
    "\n",
    "joblib.dump(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2c8106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a6f91f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "\n",
    "_, err1, _, err2 = confusion_matrix(y_test, prediction, normalize=\"all\").ravel()\n",
    "auc = roc_auc_score(y_test, probas[:, 1])\n",
    "precision = precision_score(y_test, prediction)\n",
    "recall = recall_score(y_test, prediction)\n",
    "f1 = f1_score(y_test, prediction)\n",
    "logloss = log_loss(y_test, prediction)\n",
    "\n",
    "metrics[\"err1\"] = err1\n",
    "metrics[\"err2\"] = err2\n",
    "metrics[\"auc\"] = auc\n",
    "metrics[\"precision\"] = precision\n",
    "metrics[\"recall\"] = recall\n",
    "metrics[\"f1\"] = f1\n",
    "metrics[\"logloss\"] = logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff406db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc11296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatboostModelProba(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self._model = model\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        predictions = self._model.predict(model_input)\n",
    "        predictions = np.sqrt(predictions)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a58acd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'test_model_slavuta' already exists. Creating a new version of this model...\n",
      "2026/02/05 13:54:16 INFO mlflow.tracking._model_registry.client: Waiting up to 60 seconds for model version to finish creation. Model name: test_model_slavuta, version 5\n",
      "Created version '5' of model 'test_model_slavuta'.\n"
     ]
    }
   ],
   "source": [
    "experiment_id = \"0\"\n",
    "TRACKING_SERVER_HOST = \"127.0.0.1\"\n",
    "TRACKING_SERVER_PORT = 5000\n",
    "EXPERIMENT_NAME = \"Slavuta\"\n",
    "RUN_NAME=\"model_for_registry_4\"\n",
    "REGISTRY_MODEL_NAME=\"test_model_slavuta\"\n",
    "\n",
    "mlflow.set_tracking_uri(f\"http://{TRACKING_SERVER_HOST}:{TRACKING_SERVER_PORT}\")\n",
    "mlflow.set_registry_uri(f\"http://{TRACKING_SERVER_HOST}:{TRACKING_SERVER_PORT}\") \n",
    "\n",
    "pip_requirements = \"requirements.txt\"\n",
    "signature = mlflow.models.infer_signature(X_test, prediction)\n",
    "input_example = X_test[:10]\n",
    "metadata = {\"model_type\": \"daily\"}\n",
    "\n",
    "try:\n",
    "    experiment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id\n",
    "except AttributeError:\n",
    "    experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME, experiment_id=experiment_id) as run:\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "    for k, v in metrics.items():\n",
    "        mlflow.log_metric(k, v)\n",
    "\n",
    "    model_info = mlflow.catboost.log_model(\n",
    "        registered_model_name=REGISTRY_MODEL_NAME,\n",
    "        cb_model=model,\n",
    "        artifact_path=\"models\",\n",
    "        pip_requirements=pip_requirements,\n",
    "        signature=signature,\n",
    "        input_example=input_example,\n",
    "        metadata=metadata,\n",
    "        await_registration_for=60\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b39e31d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "loaded_model = mlflow.catboost.load_model(model_uri=model_info.model_uri)\n",
    "model_predictions = loaded_model.predict(X_test)\n",
    "\n",
    "assert model_predictions.dtype == int\n",
    "\n",
    "print(model_predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d392a291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd37f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8672b7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e49ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ee943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c2336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'churn_model_nikolaistepanov_model_myown'.\n",
      "2026/02/05 13:54:32 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: churn_model_nikolaistepanov_model_myown, version 1\n",
      "Created version '1' of model 'churn_model_nikolaistepanov_model_myown'.\n"
     ]
    }
   ],
   "source": [
    "# Task to log a new model\n",
    "\n",
    "import os\n",
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "EXPERIMENT_NAME = \"churn_nikolaistepanov_myown\"\n",
    "RUN_NAME = \"model_0_registry_myonwn\"\n",
    "REGISTRY_MODEL_NAME = \"churn_model_nikolaistepanov_model_myown\"\n",
    "TRACKING_SERVER_HOST = \"127.0.0.1\"\n",
    "TRACKING_SERVER_PORT = 5000\n",
    "\n",
    "mlflow.set_tracking_uri(f\"http://{TRACKING_SERVER_HOST}:{TRACKING_SERVER_PORT}\")\n",
    "mlflow.set_registry_uri(f\"http://{TRACKING_SERVER_HOST}:{TRACKING_SERVER_PORT}\")\n",
    "\n",
    "pip_requirements = \"requirements.txt\"\n",
    "signature = mlflow.models.infer_signature(X_test, prediction)\n",
    "input_example = X_test[:10]\n",
    "metadata = {\"model_type\": \"daily\"}\n",
    "\n",
    "try:\n",
    "    experiment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id\n",
    "except AttributeError:\n",
    "    experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME, experiment_id=experiment_id) as run:\n",
    "    run_id = run.info.run_id\n",
    "    model = CatBoostClassifier(verbose=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    prediction = model.predict(X_test)\n",
    "    probas = model.predict_proba(X_test)\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    _, err1, _, err2 = confusion_matrix(y_test, prediction, normalize=\"all\").ravel()\n",
    "    auc = roc_auc_score(y_test, probas[:, 1])\n",
    "    precision = precision_score(y_test, prediction)\n",
    "    recall = recall_score(y_test, prediction)\n",
    "    f1 = f1_score(y_test, prediction)\n",
    "    logloss = log_loss(y_test, prediction)\n",
    "\n",
    "    metrics[\"err1\"] = err1\n",
    "    metrics[\"err2\"] = err2\n",
    "    metrics[\"auc\"] = auc\n",
    "    metrics[\"precision\"] = precision\n",
    "    metrics[\"recall\"] = recall\n",
    "    metrics[\"f1\"] = f1\n",
    "    metrics[\"logloss\"] = logloss\n",
    "\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    model_info = mlflow.catboost.log_model(\n",
    "        registered_model_name=REGISTRY_MODEL_NAME,\n",
    "        cb_model = model,\n",
    "        artifact_path=\"models\",\n",
    "        pip_requirements=pip_requirements,\n",
    "        signature=signature,\n",
    "        input_example=input_example,\n",
    "        metadata=metadata,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a8b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77469344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d087846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffb48ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b16e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabf19da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97f82307",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'experiment_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     15\u001b[0m client \u001b[38;5;241m=\u001b[39m mlflow\u001b[38;5;241m.\u001b[39mMlflowClient(tracking_uri\u001b[38;5;241m=\u001b[39mtracking_uri, registry_uri\u001b[38;5;241m=\u001b[39mregistry_uri)\n\u001b[1;32m     19\u001b[0m EXPERIMENT_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchurn_nikolaistepanov_myown\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 21\u001b[0m experiment_id \u001b[38;5;241m=\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEXPERIMENT_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperiment_id\u001b[49m\n\u001b[1;32m     23\u001b[0m experiment_runs \u001b[38;5;241m=\u001b[39m mlflow\u001b[38;5;241m.\u001b[39msearch_runs(\n\u001b[1;32m     24\u001b[0m     experiment_ids\u001b[38;5;241m=\u001b[39m[experiment_id],\n\u001b[1;32m     25\u001b[0m )\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'experiment_id'"
     ]
    }
   ],
   "source": [
    "# Getting results of experiments\n",
    "\n",
    "import os\n",
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "TRACKING_SERVER_HOST = \"127.0.0.1\"\n",
    "TRACKING_SERVER_PORT = 5000\n",
    "\n",
    "tracking_uri = f\"http://{TRACKING_SERVER_HOST}:{TRACKING_SERVER_PORT}\"\n",
    "registry_uri = f\"http://{TRACKING_SERVER_HOST}:{TRACKING_SERVER_PORT}\"\n",
    "\n",
    "client = mlflow.MlflowClient(tracking_uri=tracking_uri, registry_uri=registry_uri)\n",
    "\n",
    "\n",
    "\n",
    "EXPERIMENT_NAME = \"churn_nikolaistepanov_myown\"\n",
    "\n",
    "experiment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id\n",
    "\n",
    "experiment_runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    ").sort_values(by=\"start_time\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d2b971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_mle_mlflow (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
